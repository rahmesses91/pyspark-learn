{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4011d9f2",
      "metadata": {},
      "source": [
        "## 1. DataFrame Creation & Basics\n",
        "\n",
        "**Why it matters:** As a data engineer, you'll create DataFrames from various sources - APIs returning JSON, config dicts, CSV files, databases. Understanding creation methods helps you handle any data source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0fc6d74",
      "metadata": {},
      "outputs": [],
      "source": [
        "### 1.1 Creating from Dictionary\n",
        "Keys become column names, values become column data.\n",
        "\n",
        "**Why important:** Most common method when building DataFrames programmatically - API responses, transforming data structures, creating test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92113680",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Method 1: Dict of lists (column-oriented) - Most common\n",
        "orders = pd.DataFrame({\n",
        "    'order_id': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "    'customer_id': ['C1', 'C2', 'C1', 'C3', 'C2', 'C1', 'C3', 'C2'],\n",
        "    'product': ['laptop', 'phone', 'mouse', 'laptop', 'tablet', 'keyboard', 'mouse', 'laptop'],\n",
        "    'amount': [1200, 800, 25, 1200, 500, 75, 25, 1200],\n",
        "    'order_date': pd.to_datetime(['2024-01-05', '2024-01-06', '2024-01-06', \n",
        "                                   '2024-01-07', '2024-01-08', '2024-01-10',\n",
        "                                   '2024-01-12', '2024-01-15'])\n",
        "})\n",
        "orders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bafa5888",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 2: List of dicts (row-oriented) - Common from API responses\n",
        "data_rows = [\n",
        "    {'name': 'Alice', 'age': 25, 'city': 'NYC'},\n",
        "    {'name': 'Bob', 'age': 30, 'city': 'LA'},\n",
        "    {'name': 'Charlie', 'age': 35, 'city': 'Chicago'}\n",
        "]\n",
        "df_from_records = pd.DataFrame(data_rows)\n",
        "df_from_records"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1983b22c",
      "metadata": {},
      "source": [
        "### 1.3 Creating from List of Rows + Column Names\n",
        "Pass a 2D list (list of lists) as data and specify column names separately.\n",
        "\n",
        "**Why important:** Common when parsing raw data, reading from APIs that return arrays, or converting from other data formats where data and schema are separate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd3f94c2",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.14.2' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Method 3: List of rows + column names (2D list approach)\n",
        "\n",
        "# Define column names\n",
        "columns = ['customer_id', 'name', 'email', 'balance']\n",
        "\n",
        "# Define rows as list of lists\n",
        "rows = [\n",
        "    ['C001', 'John Doe', 'john@example.com', 1500.00],\n",
        "    ['C002', 'Jane Smith', 'jane@example.com', 2500.50],\n",
        "    ['C003', 'Bob Johnson', 'bob@example.com', 750.25],\n",
        "    ['C004', 'Alice Brown', 'alice@example.com', 3200.00],\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_from_rows = pd.DataFrame(data=rows, columns=columns)\n",
        "df_from_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6e3190",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variations of the same pattern\n",
        "\n",
        "# Using list of tuples (works the same as list of lists)\n",
        "columns = ['id', 'product', 'price']\n",
        "rows = [\n",
        "    ('P001', 'Laptop', 999.99),\n",
        "    ('P002', 'Mouse', 29.99),\n",
        "    ('P003', 'Keyboard', 79.99),\n",
        "]\n",
        "df_tuples = pd.DataFrame(rows, columns=columns)\n",
        "print(\"From tuples:\")\n",
        "print(df_tuples)\n",
        "\n",
        "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
        "\n",
        "# From zip() - when you have parallel lists\n",
        "ids = ['P001', 'P002', 'P003']\n",
        "products = ['Laptop', 'Mouse', 'Keyboard']\n",
        "prices = [999.99, 29.99, 79.99]\n",
        "\n",
        "# zip combines them into rows\n",
        "rows_from_zip = list(zip(ids, products, prices))\n",
        "print(f\"Zipped rows: {rows_from_zip}\")\n",
        "\n",
        "df_zipped = pd.DataFrame(rows_from_zip, columns=['id', 'product', 'price'])\n",
        "print(\"\\nFrom zip:\")\n",
        "print(df_zipped)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5912939",
      "metadata": {},
      "source": [
        "### 1.2 Creating from CSV / JSON\n",
        "Read data from files - the most common source in data pipelines.\n",
        "\n",
        "**Why important:** 90% of your data will come from files or APIs. Know the key parameters to handle encoding, delimiters, and data type issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59bfac3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# From CSV file\n",
        "# df = pd.read_csv('data.csv')\n",
        "# df = pd.read_csv('data.csv', sep=';', encoding='utf-8')  # Custom delimiter & encoding\n",
        "# df = pd.read_csv('data.csv', parse_dates=['date_col'])   # Auto-parse dates\n",
        "\n",
        "# From JSON file\n",
        "# df = pd.read_json('data.json')\n",
        "# df = pd.read_json('data.json', orient='records')  # List of dicts format\n",
        "\n",
        "# From JSON string (API response)\n",
        "# import json\n",
        "# api_response = '[{\"id\": 1, \"name\": \"test\"}]'\n",
        "# df = pd.read_json(api_response)\n",
        "\n",
        "print(\"Uncomment and modify paths to test with your files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6fc2539",
      "metadata": {},
      "source": [
        "### 1.3 Basic Attributes\n",
        "Quick properties to understand DataFrame structure without viewing data.\n",
        "\n",
        "**Why important:** Before processing, confirm you have the expected columns, correct data types, and proper indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36cccb85",
      "metadata": {},
      "outputs": [],
      "source": [
        "# shape: (rows, columns) tuple\n",
        "print(\"Shape:\", orders.shape)\n",
        "\n",
        "# columns: column names as Index\n",
        "print(\"\\nColumns:\", orders.columns.tolist())\n",
        "\n",
        "# dtypes: data type of each column\n",
        "print(\"\\nData Types:\\n\", orders.dtypes)\n",
        "\n",
        "# index: row labels\n",
        "print(\"\\nIndex:\", orders.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54eaace9",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1 Summary: DataFrame Creation & Basics\n",
        "\n",
        "| Method | What It Does | When to Use |\n",
        "|--------|--------------|-------------|\n",
        "| `pd.DataFrame(dict)` | Create from dict of lists | Building DataFrames programmatically |\n",
        "| `pd.DataFrame(list_of_dicts)` | Create from list of dicts | API responses, row-oriented data |\n",
        "| `pd.read_csv()` | Read from CSV file | Most file-based data sources |\n",
        "| `pd.read_json()` | Read from JSON file/string | API responses, config files |\n",
        "| `.shape` | Get (rows, cols) | Quick size check |\n",
        "| `.columns` | Get column names | Verify schema |\n",
        "| `.dtypes` | Get data types | Check type correctness |\n",
        "| `.index` | Get row labels | Understand indexing |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de8d457",
      "metadata": {},
      "source": [
        "## 2. Inspection & Exploration\n",
        "\n",
        "**Why it matters:** Before any data transformation, you need to understand your data - its size, types, missing values, and distribution. These functions are your first step in any data pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "477b0966",
      "metadata": {},
      "outputs": [],
      "source": [
        "### 2.1 head(n=5)\n",
        "Returns the first n rows of the DataFrame.\n",
        "\n",
        "**Why important:** Quickly preview your data after loading to verify it loaded correctly and understand the structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebaae6c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Default: first 5 rows\n",
        "orders.head()\n",
        "\n",
        "# Custom: first 3 rows\n",
        "# orders.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8f2dc09",
      "metadata": {},
      "source": [
        "### 2.2 tail(n=5)\n",
        "Returns the last n rows of the DataFrame.\n",
        "\n",
        "**Why important:** Check if data loaded completely, especially for time-series data where you want to see the most recent records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ba373d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Default: last 5 rows\n",
        "orders.tail()\n",
        "\n",
        "# Custom: last 2 rows\n",
        "# orders.tail(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80b13e8a",
      "metadata": {},
      "source": [
        "### 2.3 info()\n",
        "Prints concise summary: column names, non-null counts, data types, and memory usage.\n",
        "\n",
        "**Why important:** Essential for data quality checks - identifies missing values (null counts) and incorrect data types (e.g., dates stored as strings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9101b3c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Schema, null counts, dtypes, memory usage\n",
        "orders.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f2b9ead",
      "metadata": {},
      "source": [
        "### 2.4 describe()\n",
        "Generates descriptive statistics for numeric columns: count, mean, std, min, max, quartiles.\n",
        "\n",
        "**Why important:** Quickly spot data issues - outliers (compare min/max to mean), data entry errors, unexpected ranges. For data engineers, this validates data quality before downstream processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "171c2086",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistics for numeric columns only\n",
        "orders.describe()\n",
        "\n",
        "# Include all columns (object/categorical too)\n",
        "# orders.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ce8967f",
      "metadata": {},
      "source": [
        "### 2.5 sample(n=1)\n",
        "Returns n random rows from the DataFrame.\n",
        "\n",
        "**Why important:** For large datasets, head/tail only show edges. sample() gives unbiased view of data across the entire dataset - useful for spotting patterns or issues not visible at the beginning/end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f576c688",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random 3 rows\n",
        "orders.sample(3)\n",
        "\n",
        "# Reproducible random sample (same result every time)\n",
        "# orders.sample(3, random_state=42)\n",
        "\n",
        "# Sample 50% of rows\n",
        "# orders.sample(frac=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87cba134",
      "metadata": {},
      "source": [
        "### 2.6 shape\n",
        "Returns tuple of (rows, columns) - the dimensions of the DataFrame.\n",
        "\n",
        "**Why important:** First sanity check after loading data - did you get the expected number of rows? Helps catch truncated loads, filter mistakes, or join explosions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9576e5e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get dimensions: (rows, columns)\n",
        "print(f\"Shape: {orders.shape}\")\n",
        "print(f\"Rows: {orders.shape[0]}, Columns: {orders.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a1daf42",
      "metadata": {},
      "source": [
        "### 2.7 value_counts()\n",
        "Returns frequency count of unique values in a column (Series).\n",
        "\n",
        "**Why important:** Essential for understanding categorical data - detect dirty data (typos, inconsistent casing), unexpected values, and data distribution. Critical before groupby operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55ae5a00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count occurrences of each product\n",
        "orders['product'].value_counts()\n",
        "\n",
        "# With percentages\n",
        "# orders['product'].value_counts(normalize=True)\n",
        "\n",
        "# Include NaN in counts\n",
        "# orders['product'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df0496a",
      "metadata": {},
      "source": [
        "### 2.8 nunique()\n",
        "Returns count of unique values per column.\n",
        "\n",
        "**Why important:** Cardinality check - is this column a good join key (should be unique)? Too many categories for one-hot encoding? Detect if a column has only one value (useless for analysis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07f50ba0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unique count for all columns\n",
        "orders.nunique()\n",
        "\n",
        "# For a single column\n",
        "# orders['customer_id'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "905fb9fb",
      "metadata": {},
      "source": [
        "### 2.9 isnull().sum()\n",
        "Returns count of null/missing values per column.\n",
        "\n",
        "**Why important:** Data quality is everything in pipelines. Know exactly how many nulls exist before deciding to drop, fill, or flag them. Prevents silent failures in downstream transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fee8b58",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Null count per column\n",
        "orders.isnull().sum()\n",
        "\n",
        "# Null percentage per column\n",
        "# (orders.isnull().sum() / len(orders) * 100).round(2)\n",
        "\n",
        "# Total nulls in entire DataFrame\n",
        "# orders.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95660dda",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2 Summary: Inspection & Exploration\n",
        "\n",
        "| Function | What It Does | Why Important for Data Engineers |\n",
        "|----------|--------------|----------------------------------|\n",
        "| `head(n)` | First n rows | Verify data loaded correctly |\n",
        "| `tail(n)` | Last n rows | Check completeness, see recent records |\n",
        "| `info()` | Schema, nulls, dtypes, memory | Identify missing values & wrong types |\n",
        "| `describe()` | Stats for numeric columns | Spot outliers, validate ranges |\n",
        "| `sample(n)` | Random n rows | Unbiased view of large datasets |\n",
        "| `shape` | (rows, columns) tuple | Sanity check row counts |\n",
        "| `value_counts()` | Frequency of unique values | Understand categorical distributions |\n",
        "| `nunique()` | Count of unique values | Cardinality check for joins/encoding |\n",
        "| `isnull().sum()` | Null count per column | Data quality assessment |\n",
        "\n",
        "**Pro Tip:** Run these in order when loading new data:\n",
        "```python\n",
        "df.shape → df.info() → df.isnull().sum() → df.describe() → df.head()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb91b4c5",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Practice Datasets\n",
        "\n",
        "The following CSV files are available for practicing pandas functions. These represent a **loan application system** with related tables.\n",
        "\n",
        "**Location:** `../../practice_datasets/csv/loan_applications/`\n",
        "\n",
        "| File | Rows | Columns | Description |\n",
        "|------|------|---------|-------------|\n",
        "| `loan_applications.csv` | 1001 | `application_id`, `application_date`, `applicant_name`, `channel`, `state` | Customer loan applications |\n",
        "| `credit_checks.csv` | 1001 | `credit_check_id`, `application_id`, `check_time`, `result`, `score` | Credit check results per application |\n",
        "| `loans.csv` | 599 | `loan_id`, `application_id`, `amount`, `funded_date`, `term_months` | Funded loans (subset of applications) |\n",
        "\n",
        "**Relationships:**\n",
        "- `loan_applications.application_id` → `credit_checks.application_id` (1:1)\n",
        "- `loan_applications.application_id` → `loans.application_id` (1:0..1, not all applications become loans)\n",
        "\n",
        "**Practice challenges will be added below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c7fd31",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.14.2' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Load practice datasets\n",
        "import os\n",
        "root_directory = os.path.dirname(os.path.abspath('__file__'))\n",
        "DATA_PATH = os.path.join(root_directory, \"practice_datasets/csv/loan_applications/\")\n",
        "\n",
        "applications = pd.read_csv(DATA_PATH + \"loan_applications.csv\")\n",
        "credit_checks = pd.read_csv(DATA_PATH + \"credit_checks.csv\")\n",
        "loans = pd.read_csv(DATA_PATH + \"loans.csv\")\n",
        "\n",
        "print(\"Datasets loaded:\")\n",
        "print(f\"  applications: {applications.shape}\")\n",
        "print(f\"  credit_checks: {credit_checks.shape}\")\n",
        "print(f\"  loans: {loans.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc8318b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "---\n",
        "\n",
        "## Practice Questions\n",
        "\n",
        "### Practice 1: Create DataFrame from List of Lists\n",
        "\n",
        "**Scenario:** You receive data from an external API that returns employee records as a list of lists (no column headers). Your task is to convert this into a proper DataFrame.\n",
        "\n",
        "**Given:**\n",
        "```python\n",
        "# Raw data from API (no headers)\n",
        "employee_data = [\n",
        "    ['E001', 'Alice Johnson', 'Engineering', 75000, '2021-03-15'],\n",
        "    ['E002', 'Bob Smith', 'Sales', 65000, '2020-07-22'],\n",
        "    ['E003', 'Carol Williams', 'Engineering', 82000, '2019-11-08'],\n",
        "    ['E004', 'David Brown', 'Marketing', 58000, '2022-01-10'],\n",
        "    ['E005', 'Eva Martinez', 'Sales', 71000, '2021-09-01'],\n",
        "]\n",
        "\n",
        "# Column names (from API documentation)\n",
        "column_names = ['employee_id', 'name', 'department', 'salary', 'hire_date']\n",
        "```\n",
        "\n",
        "**Tasks:**\n",
        "1. Create a DataFrame from the data and columns\n",
        "2. Convert the `hire_date` column to datetime\n",
        "3. Display the DataFrame info to verify the structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "120a6cb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Practice 1: Your solution here\n",
        "\n",
        "# Given data\n",
        "employee_data = [\n",
        "    ['E001', 'Alice Johnson', 'Engineering', 75000, '2021-03-15'],\n",
        "    ['E002', 'Bob Smith', 'Sales', 65000, '2020-07-22'],\n",
        "    ['E003', 'Carol Williams', 'Engineering', 82000, '2019-11-08'],\n",
        "    ['E004', 'David Brown', 'Marketing', 58000, '2022-01-10'],\n",
        "    ['E005', 'Eva Martinez', 'Sales', 71000, '2021-09-01'],\n",
        "]\n",
        "\n",
        "column_names = ['employee_id', 'name', 'department', 'salary', 'hire_date']\n",
        "\n",
        "# TODO: Create DataFrame from employee_data with column_names\n",
        "pd.Datadrame(data=employee_data, columns=column_names)\n",
        "\n",
        "# TODO: Convert hire_date to datetime\n",
        "\n",
        "\n",
        "# TODO: Display info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f66138c3",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary><b>Click to see solution</b></summary>\n",
        "\n",
        "```python\n",
        "# Step 1: Create DataFrame\n",
        "employees = pd.DataFrame(data=employee_data, columns=column_names)\n",
        "\n",
        "# Step 2: Convert hire_date to datetime\n",
        "employees['hire_date'] = pd.to_datetime(employees['hire_date'])\n",
        "\n",
        "# Step 3: Verify structure\n",
        "employees.info()\n",
        "print(employees)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
